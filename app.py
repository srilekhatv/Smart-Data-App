import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, Normalizer
from pandas.api.types import is_numeric_dtype, is_integer_dtype, is_object_dtype, is_categorical_dtype
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
import warnings

import base64

def get_clickable_markdown_download_link(df, text="Download Processed Dataset", filename="cleaned_dataset.csv"):
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f"""
    <a href="data:file/csv;base64,{b64}" 
       download="{filename}" 
       style="text-decoration: none; font-size: 1.1rem; color: #00cfff;">
       {text}
    </a>
    """
    return href

def inject_dark_theme_css():
    with open("style.css") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)


# Set page config
st.set_page_config(
    page_title="Smart Data & EDA",
    layout="wide",
    initial_sidebar_state="expanded"
)


inject_dark_theme_css()

# App Title & Intro

st.title("Smart Data Pre-Processing & Exploratory Data Analysis")



st.markdown(
    """
    **Load. Clean. Explore. No Code Needed.** 

    Whether you're prepping data for machine learning or just making sense of a new dataset ‚Äî this app gives you an intuitive, powerful interface to handle it all.

    **Clean. Format. Explore. Prep ‚Äî in one seamless flow:**
    - Smart data cleaning  
    - Auto date handling  
    - Visual insights  
    - Encoding & scaling made simple
    
    _Give it a spin ‚Äî your data deserves better._ 

    """
)

st.markdown("---")

# Initialize session state
if "cleaned_df" not in st.session_state:
    st.session_state.cleaned_df = None
if "original_df" not in st.session_state:
    st.session_state.original_df = None

tab1, tab2, tab3, tab4 = st.tabs([
    "Upload, Preview & Clean",
    "Exploratory Data Analysis",
    "Preprocessing & Export",
    "Modeling"
])

with tab1:

    # -------------------------------
    # üìÇ Upload, Preview & Clean
    # -------------------------------

    # File uploader
    # üìÇ File Uploader
    uploaded_file = st.file_uploader("Upload your CSV file to get started", type=["csv"])

    # üö® Reset session if file is removed
    if "uploaded_file" in st.session_state and uploaded_file is None:
        st.session_state.clear()
        st.rerun()

    # üîÅ Load file only if new upload   
    if uploaded_file is not None and uploaded_file != st.session_state.get("uploaded_file"):
        st.session_state.uploaded_file = uploaded_file
        df = pd.read_csv(uploaded_file)
        df = df.convert_dtypes().infer_objects()
        st.session_state["df"] = df
        st.session_state["cleaned_df"] = df.copy()

    # ‚úÖ Access data if available
    if "df" in st.session_state:
        df = st.session_state["df"]
        df_cleaned = st.session_state["cleaned_df"]


        # Initial dtype conversion
        df = df.convert_dtypes()
        df = df.infer_objects()

        # Create cleaned copy early so all steps can use it
        df_cleaned = df.copy()

        # -------------------------------
        # 1Ô∏è‚É£ Data Preview
        # -------------------------------
        st.subheader("Data Preview")

        col1, col2 = st.columns([6, 1])  # Wide space on left, narrow on right

        with col2:
            show_full_data = st.checkbox("Show full dataset", value=False)

        if show_full_data:
            st.dataframe(df)
        else:
            st.dataframe(df.head(50))

        # Dataset shape
        st.write(f"**Shape of dataset:** {df.shape[0]} rows √ó {df.shape[1]} columns")

        # -------------------------------
        # üßº Data Cleaning Section
        # -------------------------------

        st.subheader("Data Cleaning")
        with st.expander("Data Cleaning Tasks"):
            cleaning_task = st.selectbox("Select a cleaning task:", [
                "No Action",
                "Remove Duplicate Rows",
                "Drop Unnecessary Columns",
                "Rename Columns",
            ])

            if cleaning_task == "Remove Duplicate Rows":
                total_dupes = df_cleaned.duplicated().sum()
                if total_dupes == 0:
                    st.success("No exact duplicate rows found.")
                else:
                    st.warning(f"Found {total_dupes} exact duplicate rows.")
                    with st.expander("Preview duplicate rows"):
                        st.dataframe(df_cleaned[df_cleaned.duplicated(keep=False)].head(10))

                    dup_mode = st.radio("Method:", [
                        "Remove exact duplicates (all columns)",
                        "Remove based on specific columns"
                    ])

                    keep_option = st.selectbox("Keep which duplicate?", ["first", "last", "none"])
                    keep_value = None if keep_option == "none" else keep_option

                    if dup_mode == "Remove exact duplicates (all columns)":
                        if st.button("Remove Duplicates"):
                            df_cleaned.drop_duplicates(keep=keep_value, inplace=True)
                            st.success("Duplicates removed.")
                    else:
                        subset_cols = st.multiselect("Select columns to define duplicates:", df_cleaned.columns)
                        if st.button("Remove Duplicates by Subset") and subset_cols:
                            before = df_cleaned.shape[0]
                            df_cleaned.drop_duplicates(subset=subset_cols, keep=keep_value, inplace=True)
                            after = df_cleaned.shape[0]
                            st.success(f"Removed {before - after} duplicates based on selected columns.")

            elif cleaning_task == "Drop Unnecessary Columns":
                cols_to_drop = st.multiselect("Select columns to drop:", df_cleaned.columns)
                if cols_to_drop:
                    if st.button("Drop Selected Columns"):
                        df_cleaned.drop(columns=cols_to_drop, inplace=True)
                        st.success(f"Dropped: {', '.join(cols_to_drop)}")

            elif cleaning_task == "Rename Columns":
                st.markdown("Edit the column names below:")
                rename_map = {}
                for col in df_cleaned.columns:
                    new_name = st.text_input(f"Rename `{col}` to:", value=col, key=f"rename_{col}")
                    if new_name and new_name != col:
                        rename_map[col] = new_name
                if rename_map:
                    if st.button("Apply Renaming"):
                        df_cleaned.rename(columns=rename_map, inplace=True)
                        st.success("Renaming applied.")

            # Show top 50 cleaned rows after all cleaning operations
            st.dataframe(df_cleaned.head(50))

        # -------------------------------
        # üóìÔ∏è Date/Time Formatting
        # -------------------------------
        st.subheader("Date/Time Formatting")

        with st.expander("Detect and Standardize Date Columns", expanded=False):
            potential_dates = [col for col in df_cleaned.columns if df_cleaned[col].dtype in ["object", "string"]]

            date_candidates = []
            for col in potential_dates:
                try:
                    df_cleaned[col] = df_cleaned[col].astype(str).str.strip()
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore", UserWarning)
                        parsed = pd.to_datetime(df_cleaned[col], errors="coerce")
                    if parsed.notna().sum() > 0:
                        date_candidates.append(col)
                except Exception:
                    continue

            if not date_candidates:
                st.info("No potential datetime columns detected.")
            else:
                selected_date_cols = st.multiselect("Select columns to convert to datetime:", date_candidates)

                for col in selected_date_cols:
                    df_cleaned[col] = df_cleaned[col].astype(str).str.strip()
                    parsed = pd.to_datetime(df_cleaned[col], errors="coerce")
                    failure_rate = parsed.isna().mean()

                    if failure_rate > 0.3:
                        st.warning(f"‚ö†Ô∏è `{col}` could not be consistently parsed ‚Äî contains mixed or invalid formats.")
                    else:
                        df_cleaned[col] = parsed.dt.strftime("%Y-%m-%d")
                        st.success(f"`{col}` converted and standardized to YYYY-MM-DD format.")

                # ‚úÖ Flattened UI for extraction (not in nested expander)
                if selected_date_cols:
                    st.markdown("#### Extract Date Parts")
                    col_to_extract = st.selectbox("Select column to extract parts from:", selected_date_cols)
                    df_cleaned[col_to_extract] = pd.to_datetime(df_cleaned[col_to_extract], errors="coerce")

                    extract_year = st.checkbox("Year")
                    extract_month = st.checkbox("Month")
                    extract_day = st.checkbox("Day")
                    extract_dayofweek = st.checkbox("Day of Week")
                    extract_hour = st.checkbox("Hour")

                    if extract_year:
                        df_cleaned[f"{col_to_extract}_Year"] = df_cleaned[col_to_extract].dt.year
                    if extract_month:
                        df_cleaned[f"{col_to_extract}_Month"] = df_cleaned[col_to_extract].dt.month
                    if extract_day:
                        df_cleaned[f"{col_to_extract}_Day"] = df_cleaned[col_to_extract].dt.day
                    if extract_dayofweek:
                        df_cleaned[f"{col_to_extract}_Weekday"] = df_cleaned[col_to_extract].dt.day_name()
                    if extract_hour:
                        df_cleaned[f"{col_to_extract}_Hour"] = df_cleaned[col_to_extract].dt.hour

                    st.success("Date parts extracted.")
                    st.markdown("#### Preview After Extraction")
                    st.dataframe(df_cleaned.head())

        st.session_state.cleaned_df = df_cleaned.copy()

 
    # TODO: File uploader, preview, cleaning logic

with tab2:

    df_cleaned = st.session_state.get("cleaned_df")
    if df_cleaned is None:
        st.warning("Please upload and clean a dataset in Tab 1 first.")
        st.stop()

    st.subheader("Dataset Summary")

    
    # Column types
    st.dataframe(df_cleaned.dtypes.astype(str).rename("Data Type"))

    # Missing values
    st.subheader("Missing Values")
    
    missing = df_cleaned.isnull().sum()
    missing = missing[missing > 0]
    if not missing.empty:
        st.dataframe(missing.rename("Missing Values"))
    else:
        st.success("No missing values found in the dataset.")

    # Categorical and Numerical feature breakdown
    st.subheader("Column Breakdown")
    cat_cols = [col for col in df_cleaned.columns if is_object_dtype(df_cleaned[col]) or isinstance(df_cleaned[col].dtype, pd.CategoricalDtype) or df_cleaned[col].dtype.name == "string"]
    num_cols = [col for col in df_cleaned.columns if pd.api.types.is_numeric_dtype(df_cleaned[col])]
    st.write(f"**Numerical Columns ({len(num_cols)}):**", num_cols)
    st.write(f"**Categorical Columns ({len(cat_cols)}):**", cat_cols)

    # Descriptive statistics
    st.subheader("Descriptive Statistics")
    st.dataframe(df_cleaned.describe().astype(str))

    
    # -------------------------------
    # üìä Correlation Heatmap
    # -------------------------------
    st.subheader("Correlation Heatmap (Numerical Features Only)")

    num_cols = [col for col in df_cleaned.columns if pd.api.types.is_numeric_dtype(df_cleaned[col])]

    if len(num_cols) > 1:
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.heatmap(df_cleaned[num_cols].corr(), annot=True, cmap="coolwarm", ax=ax)
        st.pyplot(fig)
    else:
        st.info("Not enough numerical columns to generate a heatmap.")


    

    # -------------------------------
    # 3Ô∏è‚É£ Visual EDA
    # -------------------------------
    st.subheader("Visual EDA")

    # Load cleaned dataset from session state
    df_cleaned = st.session_state.get("cleaned_df")

    col1, col2 = st.columns([1, 2])

    # LEFT column: dropdowns
    with col1:
        plot_type = st.selectbox("Select Plot Type", [
            "Histogram", "Boxplot", "Countplot", "Scatter Plot", "Line Plot", "Pairplot", "Heatmap"
        ])

        # Count how many rows of inputs are being shown
        dropdown_count = 1
        column_for_plot = None

        if plot_type in ["Histogram", "Boxplot"]:
            column_for_plot = st.selectbox("Select column", df_cleaned.select_dtypes(include=['float64', 'int64']).columns)
            dropdown_count += 1

        elif plot_type == "Countplot":
            column_for_plot = st.selectbox("Select column", df_cleaned.select_dtypes(include=['object', 'category', 'string']).columns)
            dropdown_count += 1

        elif plot_type in ["Scatter Plot", "Line Plot"]:
            x_col = st.selectbox("X-axis", df_cleaned.select_dtypes(include=['float64', 'int64']).columns, key="x_axis")
            y_col = st.selectbox("Y-axis", df_cleaned.select_dtypes(include=['float64', 'int64']).columns, key="y_axis")
            dropdown_count += 2


    # RIGHT column: apply dynamic top margin using custom CSS
    with col2:
        padding_px = 10 * dropdown_count  # Adjust multiplier as needed (50px/dropdown = safe baseline)

        st.markdown(
            f"""<div style='margin-top: {padding_px}px;'>""",
            unsafe_allow_html=True
        )

        if plot_type != "Pairplot":
            fig, ax = plt.subplots(figsize=(6, 4))

            if plot_type == "Histogram":
                sns.histplot(df_cleaned[column_for_plot], kde=True, ax=ax)
                ax.set_title(f"Histogram of {column_for_plot}")

            elif plot_type == "Boxplot":
                sns.boxplot(x=df_cleaned[column_for_plot], ax=ax)
                ax.set_title(f"Boxplot of {column_for_plot}")

            elif plot_type == "Countplot":
                sns.countplot(x=df_cleaned[column_for_plot], ax=ax)
                ax.set_title(f"Countplot of {column_for_plot}")
                plt.xticks(rotation=45)

            elif plot_type == "Scatter Plot":
                sns.scatterplot(x=df_cleaned[x_col], y=df[y_col], ax=ax)
                ax.set_title(f"Scatter Plot: {x_col} vs {y_col}")

            elif plot_type == "Line Plot":
                sns.lineplot(x=df_cleaned[x_col], y=df[y_col], ax=ax)
                ax.set_title(f"Line Plot: {x_col} vs {y_col}")
                plt.xticks(rotation=45)

            elif plot_type == "Heatmap":
                sns.heatmap(df_cleaned.select_dtypes(include=['float64', 'int64']).corr(), annot=True, cmap='coolwarm', ax=ax)
                ax.set_title("Correlation Heatmap")

            st.pyplot(fig)
            st.markdown("</div>", unsafe_allow_html=True)

        else:
            st.info("Generating pairplot...")
            pairplot_fig = sns.pairplot(df_cleaned.select_dtypes(include=['float64', 'int64']))
            pairplot_fig.fig.set_size_inches(8, 6)
            st.pyplot(pairplot_fig.fig)

    # Save updates to session state
    st.session_state.cleaned_df = df_cleaned.copy()
    # TODO: Visualizations and correlation matrix

with tab3:

    df_cleaned = st.session_state.get("cleaned_df")
    if df_cleaned is None:
        st.warning("Please upload and clean a dataset in Tab 1 first.")
        st.stop()

    st.subheader("Data Preprocessing")

    # -------------------------------
    # Handle Missing Values
    # -------------------------------

    with st.expander("Handle Missing Values", expanded=False):
        missing_option = st.selectbox(
        "Choose a missing value strategy:",
        ["No Action", "Drop rows with missing values", "Impute missing values"]
        )
        if missing_option == "Drop rows with missing values":
            df_cleaned.dropna(inplace=True)
            st.success("Rows with missing values dropped.")

        elif missing_option == "Impute missing values":
            impute_method = st.radio("Select imputation method:", ["Mean", "Median", "Mode"])

            try:
                for col in df_cleaned.columns:
                    if df_cleaned[col].isnull().sum() > 0:
                        if is_numeric_dtype(df_cleaned[col]):
                            if impute_method == "Mean":
                                val = df_cleaned[col].mean()
                                if is_integer_dtype(df_cleaned[col]):
                                    st.caption(f"Note: Rounded mean for `{col}` to fit Int64 column.")
                                    val = int(round(val))
                                df_cleaned[col] = df_cleaned[col].fillna(val)
                                st.write(f"`{col}` imputed with **mean**: {val}")

                            elif impute_method == "Median":
                                val = df_cleaned[col].median()
                                if is_integer_dtype(df_cleaned[col]):
                                    st.caption(f"Note: Rounded median for `{col}` to fit Int64 column.")
                                    val = int(round(val))
                                df_cleaned[col] = df_cleaned[col].fillna(val)
                                st.write(f"`{col}` imputed with **median**: {val}")

                            elif impute_method == "Mode":
                                val = df_cleaned[col].mode()[0]
                                df_cleaned[col] = df_cleaned[col].fillna(val)
                                st.write(f"`{col}` imputed with **mode**: {val}")
                        else:
                            val = df_cleaned[col].mode()[0]
                            df_cleaned[col] = df_cleaned[col].fillna(val)
                            st.write(f"`{col}` (non-numeric) imputed with **mode**: {val}")
                st.success(f"Missing values imputed using {impute_method.lower()}.")
            except Exception as e:
                st.error(f"Imputation failed: {str(e)}")

        # Check after handling
        st.markdown("###### Missing Values After Handling")

        missing_after = df_cleaned.isnull().sum()
        missing_percent_after = (missing_after / len(df_cleaned)) * 100
        missing_df_after = pd.DataFrame({
            "Missing Values": missing_after,
            "Percent (%)": missing_percent_after
        })
        missing_df_after = missing_df_after[missing_df_after["Missing Values"] > 0]

        if not missing_df_after.empty:
            st.dataframe(missing_df_after.astype(str))
        else:
            st.success("No missing values remaining in the cleaned dataset!")

    # Save update
    st.session_state.cleaned_df = df_cleaned.copy()

    # -------------------------------
    # Feature Scaling
    # -------------------------------
    
    with st.expander("Feature Scaling", expanded=False):
        scaling_option = st.selectbox("Choose a scaling method:", [
            "No Action", "StandardScaler", "MinMaxScaler", "RobustScaler", "MaxAbsScaler", "Normalizer"
        ])

        try:
            if scaling_option != "No Action":
                scaler_map = {
                    "StandardScaler": StandardScaler(),
                    "MinMaxScaler": MinMaxScaler(),
                    "RobustScaler": RobustScaler(),
                    "MaxAbsScaler": MaxAbsScaler(),
                    "Normalizer": Normalizer()
                }

                scaler = scaler_map[scaling_option]

                numeric_cols = [col for col in df_cleaned.columns if is_numeric_dtype(df_cleaned[col])]

                if numeric_cols:
                    df_cleaned[numeric_cols] = scaler.fit_transform(df_cleaned[numeric_cols])
                    st.success(f"Applied {scaling_option} to numeric columns.")
                    st.write("Scaled columns:", numeric_cols)

                    st.markdown("#### Preview After Scaling")
                    st.dataframe(df_cleaned.head())
                else:
                    st.warning("No numeric columns found for scaling.")

        except Exception as e:
            st.error(f"Scaling failed: {str(e)}")

        # ‚úÖ Save updated state
        st.session_state.cleaned_df = df_cleaned.copy()


    # -------------------------------
    # Categorical Encoding
    # -------------------------------

    with st.expander("Categorical Encoding", expanded=False):
        cat_cols = [
            col for col in df_cleaned.columns 
            if is_object_dtype(df_cleaned[col]) 
            or isinstance(df_cleaned[col].dtype, pd.CategoricalDtype) 
            or df_cleaned[col].dtype.name == "string"
        ]

        if not cat_cols:
            st.info("No categorical columns detected.")
        else:
            selected_encoding = st.selectbox(
                "Select encoding method:",
                ["No Action", "Label Encoding", "Ordinal Encoding", "One-Hot Encoding"]
            )

            selected_columns = st.multiselect(
                "Select categorical columns to encode:",
                cat_cols
            )

            if selected_encoding == "Ordinal Encoding":
                st.markdown("*Optional: Enter custom order for each selected column (comma-separated). Leave blank for default alphabetical order.*")

            try:
                for col in selected_columns:
                    if selected_encoding == "Label Encoding":
                        le = LabelEncoder()
                        df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))
                        st.write(f"`{col}` encoded with **Label Encoding**.")

                    elif selected_encoding == "Ordinal Encoding":
                        custom_order = st.text_input(f"Custom order for `{col}` (comma-separated)", key=f"order_{col}")
                        if custom_order:
                            categories = [x.strip() for x in custom_order.split(",")]
                            oe = OrdinalEncoder(categories=[categories])
                            df_cleaned[col] = oe.fit_transform(df_cleaned[[col]].astype(str))
                            st.write(f"`{col}` encoded with **Ordinal Encoding (custom order)**: {categories}")
                        else:
                            sorted_categories = sorted(df_cleaned[col].dropna().unique())
                            oe = OrdinalEncoder(categories=[sorted_categories])
                            df_cleaned[col] = oe.fit_transform(df_cleaned[[col]].astype(str))
                            st.write(f"`{col}` encoded with **Ordinal Encoding (default)**: {sorted_categories}")

                    elif selected_encoding == "One-Hot Encoding":
                        df_cleaned = pd.get_dummies(df_cleaned, columns=[col], drop_first=True)
                        st.write(f"`{col}` encoded with **One-Hot Encoding**.")

                if selected_encoding != "No Action" and selected_columns:
                    st.success("Encoding completed.")
                    st.markdown("#### Preview After Encoding")
                    st.dataframe(df_cleaned.head(10))

            except Exception as e:
                st.error(f"Encoding failed: {str(e)}")

        # ‚úÖ Update session state
        st.session_state.cleaned_df = df_cleaned.copy()

    # Download Dataset
    if "cleaned_df" in st.session_state:
        st.markdown(get_clickable_markdown_download_link(
            st.session_state.cleaned_df, 
            text="### Download Clean Dataset"
            ), unsafe_allow_html=True)

    
# TODO: Missing value handling, scaling, encoding, download

with tab4:
    st.info("Modeling section coming soon...")
    # TODO: Future model training and evaluation

    
   



